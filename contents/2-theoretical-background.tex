\section{Theoretical background}

\subsection{Group theory}
    \textbf{Group} $G$ is a set together with binary operation $\left(G, \bullet \right)$ satisfying
    following axioms:
    \begin{enumerate}
        \item Closure: for any $g, h \in G$, $g \bullet h \in G$
        \item Associativity:  $g\bullet \left(h \bullet j \right) =
                    \left(g \bullet h \right) \bullet j$ for any $g,h,j \in G$
        \item Identity: there exists a neutral element $e$ such that
                $e \bullet g = g \bullet e = g$ for any $g \in G$
        \item Inverse element: for every $g \in G$ exists element $h \in G$ such that
                $g \bullet h = h\bullet g = e$. The element inverse to $g$ is usually
                written as $g^{-1}$
    \end{enumerate}
    \par Group $G$ might also posses additional structure. A \textbf{topological group}
        is a group together with a topology on G, such that $G$'s binary operation:\\
        \hspace*{0.5cm} $\bullet: G \times G \to G, \left(g,h \right) \mapsto g\bullet h$ \\
        and inversion map: \\
        \hspace*{0.5cm} ${}^{-1}: G \to G, g \mapsto g^{-1}$ \\
        are continuous. Additionally if the underlying topological structure
        is some smooth n-manifold and both maps are smooth, the group is called
        \textbf{Lie group}. Familiar examples of Lie groups include translation group
        $\left( \mathbb{R}^n, + \right)$ - set of real vectors with addition or
        $\left(GL(n,\mathbb{R}), \cdot \right)$ - set of real $n \times n$ matrices of nonzero determinant
        with multiplication.\\\par
        Just as we can form functions between sets assigning elements of one set to
        the elements of other, it's possible to construct functions between groups.
        Typically we are interested in functions preserving to some extent the group structure,
        that is of the form.
        \begin{equation}
            f:G \to H, \hspace{1cm} f(x\bullet y) = f(x) \ast f(y)
        \end{equation}
        where $\bullet$ is the binary operation in group $G$ and $\ast$ is the binary operation
        in group $H$. Such functions are called \textbf{group homomorphisms}.
        Bijective homomorphism is called \textbf{isomorphism}.\\

        In this work we are mainly interested in groups in context of their actions
        on images. Given a group $G$ and set $X$ we define a
        \textbf{group action} of $G$ on $X$ as a function
        \begin{equation}
            \theta: G \times X \to X
        \end{equation}
        (with $\theta(g,x)$ often written as $g\cdot x$) satisfying following axioms:
        \begin{enumerate}
            \item Identity: $e \cdot x = x$ for every $x \in X$, where $e$ is the neutral element
                    of $G$
            \item Compatibility: $g \cdot \left(h \cdot x\right) =
                \left(g \bullet h \right) \cdot x$ for all $g$ and $h$ in $G$ and $x$ in $X$
        \end{enumerate}
        The simplest possible group action is the trivial group action, that is
        $g\cdot x = x$ for any $g$ and $x$. Familiar nontrivial examples include isometries
        of $\mathbb{R}^n$, e.g.
        translations - action of $\left(\mathbb{R}^n,+\right)$ given by $v \cdot x = v+x$ or
        rotations - action of $SO(n,\mathbb{R})$ given by $r \cdot x = Rx$ where
        $R$ is matrix representation of rotation and
        multiplication on the right is matrix-vector multiplication.








\subsection{Equivariant and invariant neural networks}
    \subsubsection{Equivariant and invariant maps}
    \hspace{0.5cm}
     Key properties of neural networks in regard to group actions are equivariance and invariance.
        These can be abstractly defined as follows: \par
        Let group $G$ act on sets $X$ and $Y$. We call function $f: X \rightarrow Y$ equivariant
        with respect to action of $G$ if for all $g \in G$ and $x \in X$
        \begin{equation}
            f(g \cdot x) = g \cdot f(x)
        \end{equation}
        In turn, we call $f$ invariant with respect to action of $G$
        \begin{equation}
            f(g \cdot x) = f(x)
        \end{equation}
        In fact invariance is a special case of equivariance where action of $G$ on $Y$ is trivial.
        It's however conceptually cleaner to make distinction between both properties.
        Intuitively equivariance tells us that when input changes, the output changes accordingly
        and invariance -- that action of G has no effect on output.\par
            If we want to construct a neural network $\mathcal{N}$ equivariant to some
        specified group action, we need to make sure each of it's layers is equivariant.
        If we treat $\mathcal{N}$ as a series of functions $f_0,f_1,...,f_n$, then
        \begin{align*}
            & \mathcal{N}(g \cdot x) = \\
            & f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(g\cdot x) =  \\
            & f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ \left( g \cdot f_0(x) \right) = \\
            & \cdots = \\
            & g \cdot f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(x) = \\
            & g \cdot \mathcal{N}(x)
        \end{align*}
        where $g\cdot x$ is the action of $g$ on $x$. Often we don't want the network
        to be fully equivariant; the desired extent of equivariance depends on the task.
        For problems of image-to-image type like segmentation full equivariance is desirable.
        However for problems where the input data gets more and more ``squished'' in consecutive layers,
        we need to break equivariance at some point. Like for example in classification,
        final layers are often fully connected and not equivariant. \par
        The easiest way to assert the network is invariant to some transformation
        is making it's very first layer $f_0$ invariant, then:
        \begin{align*}
            & \mathcal{N}(g\cdot x) = \\
            & f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(g\cdot x) =  \\
            & f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(x) = \\
            & \mathcal{N}(x)
        \end{align*}
        Equivalently one cam make the couple first layers equivariant and a single layer
        following them invariant. For example if $f_0$ is equivariant and $f_1$ invariant we get:
        \begin{align*}
            & \mathcal{N}(g\cdot x) = \\
            & f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(g\cdot x) =  \\
            & f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ \left( g \cdot f_0(x) \right) = \\
            & f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(x) = \\
            & \mathcal{N}(x)
        \end{align*}
        and the network is invariant.

    \subsubsection{GCNN}
    The primary example of equivariant neural networks are convolutional neural
    networks which are (roughly) equivariant to translations. The $4$ basic
    components of CNN are convolutional layers, pooling layers, normalization
    layers and activation functions.  These common building are approximately
    equivariant to translations:
    \begin{itemize}
        \item Convolutional layer - assuming $\textit{stride}=1$, convolution is
            equivariant except for tensor's borders. The border effects can to
            some extent be fixed by padding image with values present in the
            border.  Problem arises when \textit{stride} is greater than 1. Then
            layer becomes locally invariant -- that is small translations don't
            affect the layer's output, but translations which are multiplicities
            of \textit{stride} cause equivariant behaviour. The same is true for
            max pooling layers. In average pooling local behaviour is more
            complicated, but they are also equivariant to \textit{stride}-sized
            translations.
            \item Usual normalization schemes like batch or instance
                normalization are applied pointwise so they would be perfectly
                equivariant if not for the border effects.  Translation and
                padding change the tensor's statistics like mean and standard
                deviation slightly, so the normalizations of translated and
                non-translated tensors differ by some amount.
            \item Activation functions are also applied pointwise so, they are
                equivariant outside the translated region.
    \end{itemize}
    By generalizing architecture of CNNs we would like to construct models
    equivariant and invariant to some specified set of image transformations.
    On of the earliest work on this problem appeared in \cite{cohen2016}. There
    the so called \textit{Group Equivariant Convolutional Neural Networks}
    (G-CNNs) equivariant to possibly any discrete set of transformations were
    proposed. Their main idea is lifting the tensors the networks operates on
    from 4 dimensions to 5 dimensions. Just like traditionally the last two
    dimensions encode height and width, this new dimension is supposed to
    encode information about the chosen group of transformations. The authors
    worked on group generated by rotations by $90\deg$ (4-element group) as well as rotations
    with reflections (8-element group).
    The ideas contained in this paper were then further generalized to new
    domains, new possible group of transformations and neural network
    architectures. %TODO: write about each generalization, cite and insert some
    pictures

\subsection{Image symmetries}
    \label{sec:transformations}
    In this section we define and generalize operators changing contrast,
    brightness and color balance on image where ``image'' is an array of
    floating point numbers in range $\left[0;1\right]$ of shape $\left(3, H,
    W\right)$.
    \subsubsection{Contrast}
    \newcommand\mcc{\mathcal{C}}
        The usual formula for changing image's contrast by factor of $a$ is $$
        C_a(X) = \mathit{CLIP}\left(a\cdot X + (1-a) \cdot E\left[ \frac{w_1X_r
        + w_2X_g + w_3X_b}{w_1+w_2+w_3}\right]   \right) $$ where
        $\mathit{CLIP}$ is function clipping values to $\left[0;1\right]$ range,
        $X_r, X_g$ and $X_b$ are red, green and blue channels of the image and
        the expectation on the right is mean value of grayscale version of
        image.  Grayscale is understood to be a weighted average of image's
        channels.  Changing the contrast by factor of $1$ has no effect, using
        the factor of $2$ is supposed to double the contrast, factor of $0.5$
        halves the contrast, etc.  This definition however is not suitable for
        usage in the context of neural networks.  First of all it assumes the
        tensor has 3 channels, while we would like to have a definition
        independent of number of channels. The concept of green channel or red
        channel also doesn't exist for tensors with more channels. Therefore we
        choose to generalize the definition by replacing weighted average with
        usual average. The second problem is clipping values. Working only with
        values in range $\left[0;1\right]$ at all times would make training a
        neural network a lot harder and perhaps significantly diminish it's
        performance. To avoid these effects we entirely give up on clipping
        values.  The final formula for changing contrast is following:
        \begin{equation}
            \label{contrast}
            \mathcal{C}_a(x) = ax + (1-a) \mu_x
        \end{equation}
        where $\mu_x$ is the average value of whole tensor. It's however worth
        noting, that removing the clipping changes the general behavior of the
        transformation.  While it remains more or less unchanged for values of
        $a$ close to $1$, differences grow as me move further away from identity
        transformation.
        % TODO: plot of difference between original definition and the adopted definition

        This operator possesses a number of interesting properties:
        \begin{itemize}
            \item $\mathcal{C}_a$ is linear operator on vector space of images of fixed size:
                \begin{align*}
                    & \mathcal{C}_a(x+y) =
                    a(x+y) + (1-a)\mu_{x+y} =
                    ax + ay +(1-a)\mu_x +(1-a)\mu_y =
                \mathcal{C}_a(x) + \mathcal{C}_a(y) \\
                    & \mathcal{C}_a(\lambda x) = a\lambda x + (1-a)\mu_{\lambda x} =
                    \lambda ax + (1-a)\lambda\mu_x = \lambda(ax + (1-a)\mu_x) =
                    \lambda \mathcal{C}_a(x)
                \end{align*}

            \item Under map composition, set
                $\mathcal{C} = \left\{\mathcal{C}_a | a\in \mathbb{R}_+\right\}$
                forms a Lie group isomorphic to $\left(R_+,\cdot \right)$ -- set of
                positive real numbers with multiplication:
                \begin{enumerate}
                    \item Closure:\\
                        $\mathcal{C}_a\circ \mathcal{C}_b(x) =
                \mathcal{C}_a(bx + (1-b)\mu_x) =
                abx + a(1-b)\mu_x + (1-a)\mu_{bx+(1-b)\mu_x} =
                abx + a(1-b)\mu_x + (1-a)b\mu_x + (1-a)(1-b)\mu_x=
                abx + (1-ab)\mu_x =
                \mathcal{C}_{ab}(x)$
                    \item Associativity:\\
                        $ \mcc_a \circ(\mcc_b \circ \mcc_c(x)) =
                            \mcc_a(\mcc_{bc}(x)) = \mcc_{abc}(x) =
                                \mcc_{ab}\circ\mcc_c(x) = (\mcc_a \circ \mcc_b) \circ\mcc_c(x)$
                    \item Identity:\\
                        $\mcc_1\circ\mcc_a(x) = \mcc_a\circ\mcc_1(x) =
                            \mcc_a(x)$
                    \item Inverse element:\\
                        $\mcc_a\circ\mcc_{a^{-1}}(x) = \mcc_{a^{-1}}\circ\mcc_a(x) = \mcc_1(x)$
                \end{enumerate}
                The homomorphism is simply $h: \mcc_a \mapsto a$
                $$ h(\mcc_a)h(\mcc_b) = ab = h(\mcc_{ab})$$
                It is bijective so it's isomorphism.

            \item Equation \ref{contrast} defines a group action of $\mcc$ on space of images:
                $\mcc_1 \cdot x = 1x + (1-1)\mu_x = x$ and checking the compatibility axiom
                is the same as checking the closure axiom above.
            \item $\mcc_n$ increases standard deviation of image $n$ times:\\
                $\sigma(\mcc_n(x)) = \sigma(nx + (1-n)\mu_x) = \sigma(nx) = n\sigma(x)$
        \end{itemize}


    \subsubsection{Brightness}
        Change in image brightness is defined similarly to change in contrast, except
        the part with image's mean is skipped:
        $$ \mathcal{B}_a(X) = \mathit{CLIP}\left(a\cdot X\right) $$
        Again omitting the clipping, the brightness change operator is defined simply as
        \begin{equation}
            \mathcal{B}_a(x) = ax
        \end{equation}
        All properties characterising the $\mcc$ operators listed above
        are also true for the set $\mathcal{B} = \{\mathcal{B}_a | a>0 \}$.
        It's worth noting, that in this case removing the clipping step doesn't change
        the definition so significantly. In fact both formulas agree as long as $a\leq 1$,
        that is as along as brightness is not increased.
        %TODO: insert the plot of mean value of transformed image depending on change
        % brightness
    \subsubsection{Color balance}
        Suppose we are given a 3-channel image represented as a list of channels
        $\left[ X_r, X_g, X_b\right]$. Then the operator changing the color balance
        to the balance corresponding to the temperature $T$ is defined as
        \begin{equation}
            \mathcal{W}_T \left(\left[ X_r, X_g, X_b \right]\right) =
            \left[T_r X_r, T_g X_g, T_b X_b \right]
        \end{equation}
        where $T_r, T_g, T_b$ are RGB components corresponding to temperature $T$.
        For example for $T=1000K$, $T_r, T_g$ and $T_b$ equal $1, 0.0401$ and $0$ respectively.
        $T$ ranges from $1000K$ to $40000K$ with steps of $100$
        though there is not much change after the $15000$ threshold.
        Figure \ref{fig:color_balance} shows part of possible temperature spectrum
        together with colors characterising temperatures in terms of uint8 RGB triplets.

            \begin{figure}[h]
                \includegraphics[width=\textwidth]{color_balance}
                \label{fig:color_balance}
            \end{figure}
        However unlike contrast or brightness, the color balance is inherently
        bound to colors of image, so it's very hard to come up with reasonable
        generalization to higher dimensions which would allow us to measure
        equivariance. We choose not to do that and instead only construct
        networks invariant to changes in color balance.

