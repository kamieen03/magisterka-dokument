\section{Theoretical background}

\subsection{Group theory}
    Modern literature regarding equivariant neural networks is based heavily on
    group theory. We start with review of its basic notions.
    \textbf{Group} $G$ is a set together with binary operation $\left(G, \bullet \right)$ satisfying
    following axioms:
    \begin{enumerate}
        \item Closure: for any $g, h \in G$, $g \bullet h \in G$
        \item Associativity:  $g\bullet \left(h \bullet j \right) =
                    \left(g \bullet h \right) \bullet j$ for any $g,h,j \in G$
        \item Identity: there exists a neutral element $e$ such that
                $e \bullet g = g \bullet e = g$ for any $g \in G$
        \item Inverse element: for every $g \in G$ exists element $h \in G$ such that
                $g \bullet h = h\bullet g = e$. The element inverse to $g$ is usually
                written as $g^{-1}$
    \end{enumerate}
    \par Group $G$ might also posses additional structure. A \textbf{topological group}
        is a group together with a topology on the underlying set,
        such that its binary operation:\\
        $$\bullet: G \times G \to G, \hspace*{1em} \left(g,h \right) \mapsto g\bullet h$$
        and inversion map: \\
        $${}^{-1}: G \to G, \hspace*{1em} g \mapsto g^{-1}$$
        are continuous. Additionally if the underlying topological structure
        is some smooth n-manifold and both maps are smooth, the group is called
        \textbf{Lie group}. Familiar examples of Lie groups include translation group
        $\left( \mathbb{R}^n, + \right)$ - set of real vectors with addition or
        $\left(GL(n,\mathbb{R}), \cdot \right)$ - set of real $n \times n$ matrices of nonzero determinant
        with multiplication.\\
        Every Lie group $G$ has an associated structure $\mathfrak{g}$ called \textbf{Lie
        algebra}. For our purposes we can treat it as a real vector space
        $\mathbb{R}^n$ where $n$ equals the dimension of $G$ as manifold. The
        connection between group and its Lie algebra is established through two
        maps. \textbf{Log} is a function from $G$ to $\mathfrak{g}$ and \textbf{Exp}
        is its inverse
        mapping elements of $\mathfrak{g}$ to elements of $G$.


        Just as we can form functions between sets assigning elements of one set to
        the elements of other, it's possible to construct functions between groups.
        Typically we are interested in functions preserving to some extent the group structure,
        that is of the form.
        \begin{equation}
            f:G \to H, \hspace{1cm} f(x\bullet y) = f(x) \ast f(y)
        \end{equation}
        where $\bullet$ is the binary operation in group $G$ and $\ast$ is the binary operation
        in group $H$. Such functions are called \textbf{group homomorphisms}.
        Bijective homomorphism is called \textbf{isomorphism}.\\

        In this work we are mainly interested in groups in context of their actions
        on 3D tensors. Given a group $G$ and set $X$,
        \textbf{group action} of $G$ on $X$ is defined as a function
        \begin{equation}
            \theta: G \times X \to X
        \end{equation}
        (with $\theta(g,x)$ often written as $g\cdot x$) satisfying following axioms:
        \begin{enumerate}
            \item Identity: $e \cdot x = x$ for every $x \in X$, where $e$ is the neutral element
                    of $G$
            \item Compatibility: $g \cdot \left(h \cdot x\right) =
                \left(g \bullet h \right) \cdot x$ for all $g$ and $h$ in $G$ and $x$ in $X$
        \end{enumerate}
        The simplest possible group action is the trivial group action, that is
        $g\cdot x = x$ for any $g$ and $x$. Familiar nontrivial examples include isometries
        of $\mathbb{R}^n$, e.g.
        translations - action of $\left(\mathbb{R}^n,+\right)$ given by $v \cdot x = v+x$ or
        rotations - action of $SO(n,\mathbb{R})$ given by $r \cdot x = Rx$ where
        $R$ is matrix representation of rotation and
        multiplication on the right is matrix-vector multiplication.\\
        Now suppose we are given some function (not necessarily homomorphism)
        $f:G\to \mathbb{R}^n$.
        We can define natural action of $G$ on set of such functions by:
        \begin{equation}
            (g\cdot f)(x) = f(g^{-1}\bullet x)
            \label{eq:action_on_function}
        \end{equation}
        It's a group action because:
        $$(e\cdot f)(x) = f(e^{-1}\bullet x) = f(e \bullet x) = f(x)$$
        and
        $$ ((g\bullet h) \cdot f)(x) = f((g\bullet h)^{-1} \bullet x) =
            f(h^{-1}\bullet g^{-1} \bullet x) = (h\cdot f)(g^{-1} \bullet x) =
            (g\cdot(h \cdot f))(x)$$








\subsection{Equivariant and invariant functions}
    \label{sec:theoretical_equiinv}
    \hspace{0.5cm}
     Key properties of neural networks in regard to group actions are equivariance and invariance.
        These can be abstractly defined as follows: \par
        Let group $G$ act on sets $X$ and $Y$. We call function $f: X \rightarrow Y$
        \textbf{equivariant}
        with respect to action of $G$ if for all $g \in G$ and $x \in X$
        \begin{equation}
            f(g \cdot x) = g \cdot f(x)
        \end{equation}
        In turn, we call $f$ \textbf{invariant} with respect to action of $G$ if
        \begin{equation}
            f(g \cdot x) = f(x)
        \end{equation}
        In fact invariance is a special case of equivariance where action of $G$ on $Y$ is trivial.
        It's however conceptually cleaner to make distinction between both properties.
        Intuitively equivariance tells us that when input changes, the output changes accordingly
        and invariance -- that action of $G$ has no effect on output.\par
            If we want to construct a neural network $\mathcal{N}$ equivariant to some
        specified group action, we need to make sure each of its layers is equivariant.
        If we treat $\mathcal{N}$ as a series of functions $f_0,f_1,...,f_n$ then
        \begin{align*}
            \mathcal{N}(g \cdot x) &=
            f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(g\cdot x)  \\
            &= f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ \left( g \cdot f_0(x) \right) \\
            &= \cdots \\
            &= g \cdot f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(x) \\
            &= g \cdot \mathcal{N}(x)
        \end{align*}
        where $g\cdot x$ is the action of $g$ on $x$. Often we don't want the network
        to be fully equivariant; the desired extent of equivariance depends on the task.
        For problems of image-to-image type like segmentation full equivariance
        to transformations like rotation or translation is desirable.
        However for problems where the input data gets more and more ``squished'' in consecutive layers,
        we need to break equivariance at some point. Like for example in classification,
        where final layers are often fully connected and not equivariant. \par
        The easiest way to assert the network is invariant to some transformation
        is making its very first layer $f_0$ invariant:
        \begin{align*}
            \mathcal{N}(g\cdot x)
            &= f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(g\cdot x)  \\
            &= f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(x) \\
            &= \mathcal{N}(x)
        \end{align*}
        Equivalently \cite{lie_transformer}
        one can make the couple first layers equivariant and a single layer
        following them invariant. For example if $f_0$ is equivariant and $f_1$ invariant we get:
        \begin{align*}
            \mathcal{N}(g\cdot x)
            &= f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(g\cdot x)  \\
            &= f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ \left( g \cdot f_0(x) \right) \\
            &= f_n \circ f_{n-1} \circ \cdots \circ f_1 \circ f_0(x) \\
            &= \mathcal{N}(x)
        \end{align*}
        and the network is again invariant. Modern literature is often not so
        clear regarding terminology and networks of the above type are sometimes
        also called equivariant.

\subsection{Equivariant neural networks}
    \subsubsection{GCNN}
    The primary example of equivariant neural networks are convolutional neural
    networks which are (roughly) equivariant to translations. The $4$ basic
    components of CNN are convolutional layers, pooling layers, normalization
    layers and activation functions.  These common building blocks are approximately
    equivariant to translations:
    \begin{itemize}
        \item Convolutional layer is
            equivariant outside of image's borders. The border effects can to
            some extent be fixed by padding image with values present in the
            border or their averages.
        \item MaxPooling layer is locally invariant -- that is small translations don't
            affect the layer's output, but translations which are multiplicities
            of window's size cause equivariant behaviour.
            In average pooling local behaviour is more
            complicated, but they are also equivariant to window-sized
            translations.
        \item Usual normalization schemes like batch or instance
            normalization depend on global statistics
            like mean and standard deviation which change slightly during
            translation so they are not exactly equivariant. However given
            proper padding method these changes can be made very small if
            translation is not too big.
        \item Activation functions are applied pointwise so, they are
            perfectly equivariant outside borders.
    \end{itemize}
    Of course the most crucial parts of CNN are convolution layers. At each
    layer CNN takes as input 3D tensor which can be thought of as a
    stack of functions $f:\mathbb{Z}^2\to\mathbb{R}^C$ where $C$ is number of channels.
    Convolutional layer correlates it with a set of filters $\psi^i$
    and each filter can also be thought of as a stack of functions
    $\psi^i:\mathbb{Z}^2\to\mathbb{R}^C$:
    \begin{equation}
        \label{eq:cnn}
        (f\star\psi^i)(x) = \sum_{y\in\mathbb{Z}^2}\sum_{c=1}^C
        f_c(y)\psi_{c}^{i}(y-x)
    \end{equation}

    By generalizing convolutions and architecture of CNNs we would like to
    construct models
    equivariant or invariant to some specified set of image transformations.
    One of the earliest works on this problem appeared in \cite{cohen2016}. There
    the so called \textit{Group Equivariant Convolutional Neural Networks}
    (G-CNNs) equivariant to possibly any discrete set of transformations were
    proposed. Their main idea is generalizing the convolution layer by
    lifting tensors the network operates on to higher dimensions.
    Just like traditionally the last two
    dimensions encode height and width, these new dimensions can
    encode information about the chosen group of transformations. Higher
    dimensional tensors are then thought of as stacks of functions
    $F:G\to\mathbb{R}^C$, where $G$ is the group of transformations.
    Pictures \ref{fig:p4_rot} and \ref{fig:p4m_rot} show
    feature maps for $p4$ -- group of translations and rotations and for
    $p4m$ -- group of
    translations, rotations and reflections.
    Convolutional filters of respective GCNNs are shaped in the same way
    but their heights and widths are smaller (typically $3\times3$ or $5\times5$
    just like in normal CNNs).
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth]{p4_rot}
        \caption{On the left: schematic representation of feature map $F$ flowing through
            p4-equivariant GCNN. Computationally $F$ can be thought of as tensor of shape
            $4\times H \times W$, where each of $4$ submaps represents one of
            rotations by $0\degree$, $90\degree$, $180\degree$ or $270\degree$.
            On the right, the same feature map rotated by
            $90 \degree$ counterclockwise -- $r\cdot F$ in the sense of equation
            \ref{eq:action_on_function}. Note that every submap follows the
            red arrow as well as gets rotated. This stems from the structure of
            $p4$ group. Taken from \cite{cohen2016}.}
        \label{fig:p4_rot}
    \end{figure}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth]{p4m_rot}
        \caption{Feature map of p4m-equivariant GCNN and its rotation by $r$.
                    Taken from \cite{cohen2016}.}
        \label{fig:p4m_rot}
    \end{figure}

    \textbf{Lift} operation is a particular kind of convolution also proposed in
    \cite{cohen2016}
    that transforms usual function stack $f$ on $\mathbb{Z}^2$ into
    function stack $F$ on group $G$. Computationally it takes as input
    usual 3d tensor (channel, height, width) and produces
    N-dimensional tensor with $N>3$:
    \begin{equation}
        \label{eq:gcnn-lift}
        \mathit{Lift}(f)(g) = F(g) = (f\star\psi^i)(g) = \sum_{y\in \mathbb{Z}^2}\sum_{c=1}^C
        f_c(y)\psi_{c}^{i}(g^{-1}y)
    \end{equation}
    Note that $g^{-1}y$ is an element of $G$, so filters have values defined on
    the group.
    Once the input tensor is lifted, in consecutive convolutional layers it
    undergoes \textbf{group convolutions}:
    \begin{equation}
        \label{eq:gcnn}
        F(g) = (f\star\psi^i)(g) = \sum_{y\in G}\sum_{c=1}^C
        f_c(y)\psi_{c}^{i}(g^{-1}y)
    \end{equation}
    Near the end of the network signals are back again projected to original
    dimensionality by a single \textbf{projection layer}. This layer acts like
    pooling but on the previously added group dimension and not the spatial
    dimensions. Depending on implementation it computes either sum, mean or
    maximal value. In this work we use max.

    While in equation \ref{eq:cnn} computation is done by sliding
    $y$ on $\mathbb{Z}^2$, in equation \ref{eq:gcnn} it's replaced by more
    general iteration on all group elements. If we replace $G$ by $\mathbb{Z}^2$
    and set $g=x$ we obtain equation \ref{eq:cnn} back since in $\mathbb{Z}^2$,
    $x^{-1}$ is just $-x$. We can prove operator defined by equation
    \ref{eq:gcnn} is equivariant to action of any element $h$ of $G$:
    \begin{align*}
        ((h\cdot f)\star\psi)(g) & =
        \sum_{y\in G}\sum_{c=1}^C f_c(h^{-1}y)\psi_{c}(g^{-1}y)\\
        & = \sum_{hy\in G}\sum_{c=1}^C f_c(y)\psi_{c}(g^{-1}hy)\\
        & = \sum_{hy\in G}\sum_{c=1}^C f_c(y)\psi_{c}((h^{-1}g)^{-1}y)\\
        & = (f\star\psi)(h^{-1}g) &  \\
        & = (h\cdot(f\star\psi))(g)
    \end{align*}
    Iterating over $hy$ is the same as iterating over $y$ as both mean simply
    summation over all of elements of $G$.
    The proof goes similarly for equation \ref{eq:gcnn-lift}.

\subsubsection{Related work}
    The ideas contained in \cite{cohen2016} were further generalized to new
    possible groups of transformations, domains and neural network
    architectures. \cite{cohen_spherical_cnns, kondor_trivedi,
    esteves_so3} describe convolutional layers equivariant to
    the group of 3D rotations $\mathit{SO}(3)$.
    They make heavy use of spherical harmonics for parametrization and
    speedup of computation. Equivariance to $\mathit{SO}(3)$ is beneficial
    in any tasks regarding real world environments like depth estimation or 3D
    objects classification \cite{esteves_so3}. It helps also at
    prediction of properties of
    molecules described in terms of 3D-coordinates \cite{lieconv}.
    In less obvious way
    $\mathit{SO}(3)$-equivariant GCNNs are used to process data whose natural
    domain is spherical, e.g. Earth weather data or images from drone camera
    \cite{cohen_spherical_cnns}.

    Another particularly well-researched group of transformations is scaling.
    \cite{deep_scale_spaces} formalizes problem in terms of semigroup theory and
    uses gaussian kernels together with downsampling to eliminate higher
    frequencies which would otherwise might cause artifacts in image.
    \cite{bekkers2019} constructs general architecture equivariant to any (Lie) group
    of transformations specified by programmer. The scaling is then represented abstractly as
    the group $\left(\mathbb{R}_+,\cdot\right)$. \cite{scale_steerable} uses
    fixed base of steerable filters and constructs actual filters as linear
    combinations of these.


    \cite{bekkers2019, lieconv, lie_transformer} propose general frameworks for
    GCNNs equivariant to any Lie group. In order to parametrize the manifold and
    make sense of distances on it, they use Lie algebras and exponential maps.
    This way
    instead of dealing with often unintuitive structure of Lie groups, we can
    compute everything with familiar operations on vectors. For example
    instead of computing distance between two rotation matrices directly, it
    suffices to compute the difference between the vectors corresponding to
    these matrices in the Lie algebra. \cite{lieconv} and \cite{bekkers2019} are
    described more closely in the next section \ref{sec:lie_gcnn}.

    \cite{lie_transformer, attentive_gcnn, coattentive} add attention mechanisms
    and ensure they are also equivariant with respect to chosen group of
    transformations. \cite{lie_transformer} extends ideas from \cite{lieconv}
    and is able to handle arbitrary Lie groups.
    \cite{attentive_gcnn, coattentive} operate on discrete (finite) groups of
    transformations and use circulant attention matrices to make them
    equivariant to rotations. All of these architectures require unfortunately a
    lot of memory -- one of models constructed for CIFAR10 dataset in
    \cite{attentive_gcnn} uses 72GBs of RAM.

    All the works mentioned so far specify explicitly and precisely the
    transformations the network is supposed to be equivariant to. In
    \cite{augerino} on the other hand kind of transformation is stated, but the
    network has to learn its extent on its own during usual training
    procedure, e.g. model should be equivariant to rotations from
    $\left[-\theta;\theta\right]$ interval but parameter $\theta$ is to be learned from
    data alone.


    The overall framework of GCNNs was summarized in \cite{kondor_trivedi} and
    \cite{cohen2020general} in terms of group
    theory, group actions, fourier analysis and representation theory.

    \subsubsection{Lie GCNNs}
    \label{sec:lie_gcnn}
    The models described in \cite{lieconv} and \cite{bekkers2019} operate in
    similar fashion.
    First they lift the input tensor, so that it gains a new dimension
    representing chosen group of transformations, just like in \cite{cohen2016}.
    Then the signal goes through a series of typical neural components -- convolutions,
    pooling layers, activation functions and normalization layers. The last
    three types of layers are mostly the same as in regular CNNs. While
    architecture of NN in \cite{bekkers2019} resembles traditional VGG,
    \cite{lieconv} constructs ResNet-like models. Given equivariant
    components from \cite{bekkers2019}, going from VGG to ResNet is purely a
    matter of engineering and lining up the dimensions -- if both normal and
    residual connections in ResNet are equivariant, then so is their sum.
    Mathematical formulation of convolutions is also essentially the same.
    Just like in \cite{cohen2016}, after the tensor is lifted, it takes values
    on the group and convolutional layers compute the convolution with respect
    to the group:
    \begin{equation}
        (Kf)(g) = \int_G K(g^{-1}h)f(h)d\mu(h)
        \label{eq:lie_integral}
    \end{equation}
    where $G$ is the group, $K$ is convolutional kernel, $f$ is signal and $d\mu$ is the Haar
    measure of group $G$ which makes the integration possible. However given
    the discrete nature of input signals, this integral also needs to be
    discretized and turned into a sum. This is achieved by sampling elements from $G$
    according to its Haar measure and computing sum over them:
    \begin{equation}
        (Kf)(g) = \frac{1}{N} \sum_{i=1}^N K(g^{-1}h_i)f(h_i)
        \label{eq:lie_sum}
    \end{equation}
    where $N$ is number of sampled elements. The lifting itself is done in
    pretty much the same way, except that group $G$ in this case is just square
    grid $\mathbb{Z}^2$.

    All of these elements are common to both papers. Where the works
    differ significantly is
    the method by which convolutional kernels are constructed. Equation
    \ref{eq:lie_integral} implies that kernels must have values defined for all
    elements of group $G$. This requirement doesn't hold in practice
    due to discretization so we could potentially model kernels as tensors just
    like in
    regular GCNNs. Such representation however causes increase
    in computation proportional to dimensionality (as manifold) of the
    transformation group. And within single dimension it's proportional to
    number of sample points. For this reason both papers model kernels as highly
    parametrized continuous functions. \cite{lieconv} takes conventional
    approach of deep learning -- every kernel is small neural network. It takes
    as input a vector in Lie algebra of $G$ and gives values of an associated
    elements of $G$ on output. \cite{bekkers2019} on the other hand uses fixed
    basis of functions -- so called B-splines. B-splines are smooth, strictly positive
    symmetric functions defined on all of $\mathbb{R}$. They also take as input
    vectors from lie algebra and compute output as weighted sum of differently
    centered individual B-splines. The weights are learnable parameters in this
    case. The exact form of B-splines doesn't matter that much, though they very
    closely resemble the gaussian function. More precisely if we are given
    element $g$ for which we want to compute value of the kernel K, we do so by
    equation
    \begin{equation}
        K(g) = \sum_{i=1}^N
        c_i B\left(\frac{\text{Log } g_i^{-1}g}{S}\right)
        \label{eq:bsplines}
    \end{equation}
    where Log is the logarithmic map from $G$ to its Lie algebra and $c_i$ are
    learnable parameters. Each B-spline is
    centered around some element $g_i$ and is scaled by scalar constant $S$. Note that
    $\text{Log } g_i^{-1}g$ is really a vector $\mathbf{x}$ in $\mathbb{R}^n$,
    whereas B-splines are defined for real numbers. Extension to vector spaces
    is done via multiplication:
    \begin{equation}
        B(\mathbf{x}) = B(x_1)B(x_2)\cdots B(x_n)
    \end{equation}
    where $\mathbf{x} = \left[x_1,x_2,\dots,x_n\right]$.












\subsection{Image symmetries}
    \label{sec:transformations}
    In this section we define and generalize symmetries of images that we wish
    neural networks to respect. First we describe
    properties related to lightning in image - contrast,
    brightness, color balance and gamma correction. Then geometric operators -
    rotations, scaling and shear - are presented.
    Throughout this section and for the rest of the work,
    ``image'' is understood to be an array of
    floating point numbers in range $\left[0;1\right]$ of shape $\left(3, H,
    W\right)$.
    \subsubsection{Contrast}
    \newcommand\mcc{\mathcal{C}}
        Though notion of contrast has many different definitions and variations,
        one popular formula \cite{torch_contrast, tf_contrast}
        for its adjustment by factor of $a$ is
        \begin{equation}
            \label{eq:contrast_old}
        C_a(X) = \mathit{CLIP}\left(a\cdot X + (1-a) \cdot E\left[ \frac{w_1X_r
        + w_2X_g + w_3X_b}{w_1+w_2+w_3}\right]   \right)
        \end{equation}
        where
        $\mathit{CLIP}$ is function clipping values to $\left[0;1\right]$ range,
        $X_r, X_g$ and $X_b$ are red, green and blue channels of the image and
        the expectation on the right is mean value of grayscale version of
        image.  Grayscale is understood to be a weighted average of image's
        channels.  Changing the contrast by factor of $1$ has no effect, using
        the factor of $2$ is supposed to double the contrast, factor of $0.5$
        halves the contrast, etc.  This definition however is not suitable for
        usage in the context of neural networks.  First of all it assumes the
        tensor has 3 channels, while we would like to have a definition
        independent of number of channels. The concept of green channel or red
        channel also doesn't exist for tensors with more channels. Therefore we
        choose to generalize the definition by replacing weighted average with
        usual average. The second problem is clipping values. Working only with
        values in range $\left[0;1\right]$ at all times would make training a
        neural network a lot harder and perhaps significantly diminish its
        performance. To avoid these effects we entirely give up on clipping
        values.  The final formula for changing contrast is following:
        \begin{equation}
            \label{eq:contrast}
            \mathcal{C}_a(x) = ax + (1-a) \mu_x
        \end{equation}
        where $\mu_x$ is the average value of whole tensor. It's worth
        noting, that removing the clipping changes the general behavior of the
        transformation.  While it remains more or less unchanged for values of
        $a$ close to $1$, differences grow as me move further away from identity
        transformation.

        This operator possesses a number of interesting properties:
        \begin{itemize}
            \item $\mathcal{C}_a$ is linear operator on vector space of images of fixed size:
                \begin{align*}
                    & \mathcal{C}_a(x+y) =
                    a(x+y) + (1-a)\mu_{x+y} =
                    ax + ay +(1-a)\mu_x +(1-a)\mu_y =
                \mathcal{C}_a(x) + \mathcal{C}_a(y) \\
                    & \mathcal{C}_a(\lambda x) = a\lambda x + (1-a)\mu_{\lambda x} =
                    \lambda ax + (1-a)\lambda\mu_x = \lambda(ax + (1-a)\mu_x) =
                    \lambda \mathcal{C}_a(x)
                \end{align*}

            \item Under map composition, set
                $\mathcal{C} = \left\{\mathcal{C}_a | a\in \mathbb{R}_+\right\}$
                forms a Lie group isomorphic to $\left(\mathbb{R}_+,\cdot \right)$ -- set of
                positive real numbers with multiplication:
                \begin{enumerate}
                    \item Closure:\\
                        $\mathcal{C}_a\circ \mathcal{C}_b(x) =
                \mathcal{C}_a(bx + (1-b)\mu_x) =
                abx + a(1-b)\mu_x + (1-a)\mu_{bx+(1-b)\mu_x} =
                abx + a(1-b)\mu_x + (1-a)b\mu_x + (1-a)(1-b)\mu_x=
                abx + (1-ab)\mu_x =
                \mathcal{C}_{ab}(x)$
                    \item Associativity:\\
                        $ \mcc_a \circ(\mcc_b \circ \mcc_c(x)) =
                            \mcc_a(\mcc_{bc}(x)) = \mcc_{abc}(x) =
                                \mcc_{ab}\circ\mcc_c(x) = (\mcc_a \circ \mcc_b) \circ\mcc_c(x)$
                    \item Identity:\\
                        $\mcc_1\circ\mcc_a(x) = \mcc_a\circ\mcc_1(x) =
                            \mcc_a(x)$
                    \item Inverse element:\\
                        $\mcc_a\circ\mcc_{a^{-1}}(x) = \mcc_{a^{-1}}\circ\mcc_a(x) = \mcc_1(x)$
                \end{enumerate}
                The homomorphism is simply $h: \mcc_a \mapsto a$
                $$ h(\mcc_a)h(\mcc_b) = ab = h(\mcc_{ab})$$
                It is bijective so it's isomorphism.

            \item Equation \ref{eq:contrast} defines a group action of $\mcc$ on space of images:
                $\mcc_1 \cdot x = 1x + (1-1)\mu_x = x$ and checking the compatibility axiom
                is the same as checking the closure axiom above.
            \item $\mcc_n$ increases standard deviation of image $n$ times:\\
                $\sigma(\mcc_n(x)) = \sigma(nx + (1-n)\mu_x) = \sigma(nx) = n\sigma(x)$
        \end{itemize}
        \par Visually the difference between operators using weighted and unweighted
        averages is small. Picture \ref{fig:contrast_pics} shows series of
        photographies from STL10 dataset with with varying contrast changes.

        \newcommand{\contrastpic}[3]{
            \begin{subfigure}{#3\textwidth}
                \includegraphics[width=\linewidth]{contrast_#1/#2}
            \end{subfigure}}
        \newcommand{\contrastpiccaption}[4]{
            \begin{subfigure}{#3\textwidth}
                \includegraphics[width=\linewidth]{{contrast_#1/#2}.jpg}
                \caption*{$a=#4$}
            \end{subfigure}}

        \begin{figure}[h]
            \centering
            \foreach \n in {0_1, 0_25, 0_5, 1, 2, 4,10}
                {\contrastpic{default}{0temp\n}{0.12}} \\
            \foreach \n in {0_1, 0_25, 0_5, 1, 2, 4,10}
                {\contrastpic{custom}{0temp\n}{0.12}} \\[1em]
            \foreach \n in {0_1, 0_25, 0_5, 1, 2, 4,10}
                {\contrastpic{default}{121temp\n}{0.12}} \\
            \foreach \n in {0_1, 0_25, 0_5, 1, 2, 4,10}
                {\contrastpic{custom}{121temp\n}{0.12}} \\[1em]
            \foreach \n in {0_1, 0_25, 0_5, 1, 2, 4,10}
                {\contrastpic{default}{105temp\n}{0.12}} \\
            \foreach \n in {0.1, 0.25, 0.5, 1, 2, 4,10}
                {\contrastpiccaption{custom}{105temp\n}{0.12}{\n}}

        \caption{Differences between weighted and unweighted contrast change
            operator. Top rows: $C_a$ operator (equation \ref{eq:contrast_old}).
            Bottom rows: $\mcc_a$ operator (equation \ref{eq:contrast} with
            clipping).}
        \label{fig:contrast_pics}
        \end{figure}


    \subsubsection{Brightness}
        Change in image brightness is defined similarly to change in contrast, except
        the subtraction of image's mean is skipped:
        \begin{equation}
            \label{eq:bright_old}
            B_a(X) = \mathit{CLIP}\left(aX\right)
        \end{equation}
        Again omitting the clipping, the brightness change operator is defined simply as
        \begin{equation}
            \mathcal{B}_a(x) = ax
            \label{eq:bright}
        \end{equation}
        All properties characterising the $\mcc$ operators listed above
        are also true for the set $\mathcal{B} = \{\mathcal{B}_a | a>0 \}$.
        It's worth noting, that in this case removing the clipping step doesn't change
        the definition so significantly. In fact both formulas agree as long as $a\leq 1$,
        that is as along as brightness is not increased.
        Picture \ref{fig:brightness_pics} shows images with brightness varying
        according to equation \ref{eq:bright_old}.


        \newcommand{\brightnesspic}[2]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{brightness/#1}.jpg}
            \end{subfigure}}
        \newcommand{\brightnesspiccaption}[3]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{brightness/#1}.jpg}
                \caption*{$a=#3$}
            \end{subfigure}}

        \begin{figure}[h]
            \centering
            \foreach \n in {0.1, 0.25, 0.5, 1, 2, 4,10}
                {\brightnesspic{0temp\n}{0.12}} \\
            \foreach \n in {0.1, 0.25, 0.5, 1, 2, 4,10}
                {\brightnesspic{121temp\n}{0.12}} \\
            \foreach \n in {0.1, 0.25, 0.5, 1, 2, 4,10}
                {\brightnesspiccaption{105temp\n}{0.12}{\n}}

        \caption{Images from STL10 dataset with brightness changed with operator
            $B_a$.}
        \label{fig:brightness_pics}
        \end{figure}



    \subsubsection{Color balance}
        Suppose we are given a 3-channel image represented as a list of channels
        $\left[ X_r, X_g, X_b\right]$. Then the operator changing the color balance
        to the balance corresponding to the temperature $T$ is defined as
        \begin{equation}
            \mathcal{W}_T \left(\left[ X_r, X_g, X_b \right]\right) =
            \left[T_r X_r, T_g X_g, T_b X_b \right]
            \label{eq:color}
        \end{equation}
        where $T_r, T_g, T_b$ are RGB components corresponding to temperature $T$.
        For example for $T=1000K$, $T_r, T_g$ and $T_b$ equal $1, 0.0401$ and $0$ respectively
        \cite{imgaug_color_balance}.
        $T$ ranges from $1000K$ to $40000K$ with steps of $100K$
        though there is not much change after the $15000K$ threshold.
        Figure \ref{fig:color_balance} shows part of possible temperature spectrum
        together with colors characterising temperatures in terms of uint8 RGB triplets.
        Picture \ref{fig:cb_pics} shows images with varying color balance.

        \begin{figure}[h]
            \includegraphics[width=\textwidth]{color_balance}
            \caption{Part of spectrum of color balance temperatures and colors
            with steps of 200K.}
            \label{fig:color_balance}
        \end{figure}

        \newcommand{\cbpic}[2]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{color_balance/#1}.jpg}
            \end{subfigure}}
        \newcommand{\cbpiccaption}[3]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{color_balance/#1}.jpg}
                \caption*{$T=#3K$}
            \end{subfigure}}

        \begin{figure}[h]
            \centering
            \foreach \n in {1000, 3000, 5000, 7000, 9000}
                {\cbpic{0temp\n}{0.18}} \\
            \foreach \n in {1000, 3000, 5000, 7000, 9000}
                {\cbpic{121temp\n}{0.18}} \\
            \foreach \n in {1000, 3000, 5000, 7000, 9000}
                {\cbpiccaption{105temp\n}{0.18}{\n}}

        \caption{Images from STL10 dataset with color balance changed with operator
            $\mathcal{W}_T$.}
        \label{fig:cb_pics}
        \end{figure}

        Unlike contrast or brightness, the color balance is inherently
        bound to colors of image, so it's very hard to come up with reasonable
        generalization to higher dimensions which would allow us to measure
        equivariance. We choose not to do that and instead only construct
        networks invariant to changes in color balance.

    \subsubsection{Gamma correction}
        In virtually all computer vision libraries \cite{skimage_gamma,
        cv_gamma, torch_gamma}
        gamma correction of image X is defined as
        \begin{equation}
            G_{a,c}(X) = cX^a
        \label{eq:simple_gamma}
        \end{equation}
        where $c$ and $a \in \mathbb{R}_+$ and exponentiation is done entrywise.
        Multiplication by $c$ is often optional and we drop it for simplicity.
        Figure \ref{fig:gamma_pics} shows examples of pictures with various gamma
        settings.

        \newcommand{\gammapic}[2]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{gammas/#1}.jpg}
            \end{subfigure}}
        \newcommand{\gammapiccaption}[3]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{gammas/#1}.jpg}
                \caption*{$a=#3$}
            \end{subfigure}}

        \begin{figure}[h]
            \centering
            \foreach \n in {0.33, 0.75, 1, 1.5, 3}
                {\gammapic{0temp\n}{0.18}} \\
            \foreach \n in {0.33, 0.75, 1, 1.5, 3}
                {\gammapic{121temp\n}{0.18}} \\
            \foreach \n in {0.33, 0.75, 1, 1.5, 3}
                {\gammapiccaption{105temp\n}{0.18}{\n}}

            \caption{Images from STL10 dataset with varying gamma settings.}
        \label{fig:gamma_pics}
        \end{figure}


        Unfortunately this definition is troublesome when applied to tensors
        with negative values as for example $a=\frac{1}{2}$ results in
        complex values. Also since every number has $n$ complex roots of degree
        $n$, a problem with selecting the proper root appears. In order to avoid
        these problems, we restrict computation to real numbers and generalize
        equation \ref{eq:simple_gamma} by:
        \newcommand\mcg{\mathcal{G}}
        \begin{equation}
            \mathcal{G}_a(X) = s(X)\cdot|X|^a
            \label{eq:gamma}
        \end{equation}
        where $s(X)$ is a tensor of the same shape as $X$ containing
        signs of its entries and $|X|$ contains absolute values of entries.
        Exponentiation is again computed entrywise. This way we separate signs
        and magnitudes.
        \par Unlike the three previous operators, $\mathcal{G}_a$ is nonlinear --
        $(X+Y)^a \neq X^a + Y^a$ unless $a=1$. This family of transformations forms however a
        group under function composition $\mcg_a \circ \mcg_b = \mcg_{ab}$.
        It's isomorphic to $(\mathbb{R}_+, \cdot)$ with isomorphism $f$
        given by $f:\mcg_a \mapsto a$ and acts on the space of
        tensors of fixed shape:
        \begin{itemize}
            \item $\mathcal{G}_1(X) = X^1 = X$
            \item $\mcg_a\left(\mcg_b\left(X\right)\right) =
                \mcg_a\left(s\left(X\right)\cdot\left|X\right|^b\right) =
                s\left(s\left(X\right)\cdot\left|X\right|^b\right)
                \cdot\left|s\left(x\right)\cdot\left|X\right|^b\right|^a =
                s\left(s\left(X\right)\right)\cdot\left|\left|X\right|^b\right|^a
                = s\left(X\right)\cdot\left|X\right|^{ba} = \mcg_{ab}(X)$
        \end{itemize}



    \subsubsection{Rotation}
        When speaking of image rotating, we typically mean rotation around
        image's center. In coordinate system with $(0,0)$ representing the
        center, rotation of coordinates
        can be represented as an element of $SO(2)$ group - $2\times2$ matrix of
        the form
        $$\mathcal{R}_\theta = \begin{bmatrix}
                \cos\theta & -\sin\theta \\
                \sin\theta &  \cos\theta
            \end{bmatrix}$$
        which transforms coordinates (2d vectors) by matrix multiplication.
        After rotating coordinates, pixels of original image need to be resampled
        in some way, which slightly distorts the picture unless the rotation angle
        is a multiple of $90\degree$. If we want rotations to preserve all
        information present in picture
        we need to take care of appropriate
        expansion of borders and possible padding with 0s.
        It's also important from more theoretical point of view - expansion
        assures rotations compose nicely, that is form a group
        action on the space of images.
        Figure
        \ref{fig:rotation_pics} illustrates necessity of border expansion.
        Lack of expansion causes rotation composition to lose part of
        information which moves outside the border.

        \begin{figure}[h]
            \centering
            \includegraphics[width=\textwidth]{rotations}
            \caption{Difference between rotation with and without border
                expansion. Left column shows the original image and its
                rotation by 90\degree. In middle column image is first rotated
                by 45\degree with expansion, then rotated again and cropped to
                original size. In the right column picture is simply rotated
                twice by 45\degree.}
            \label{fig:rotation_pics}
        \end{figure}
        While filters present in GCNNs are higher dimensional than in regular
        CNNs, spatial interpretation of the last two dimensions remains
        unchanged. Therefore it makes sense to generalize $\mathcal{R}$ to
        transform N-dimensional tensor by separately rotating last two
        dimensions of every subtensor and stacking them in original shape.
        The same goes for scaling and shear transformations described next.

    \subsubsection{Scaling}
        Scaling is a zoom-in or zoom-out transformation in reference to the
        center of the image. Scaling of coordinates can be written as scalar
        matrix
        \begin{equation}
            \mathcal{S}_k = \begin{bmatrix}
                k & 0 \\
                0 & k
            \end{bmatrix}
        \end{equation}
        Just like in rotation, after rescaling of coordinates, image needs to be
        resampled.
        Figure \ref{fig:scalings} shows varying degree of scaling.
        Note that by default scaling-up causes loss of
        information, so we might want to use image expansion as well.
        However while using finite expansion (increasing image size by at most
        $\sqrt{2}$) we can make rotations into a group action,
        it's not the case for scaling. In order to preserve all information
        during scaling by factor of $k>1$, we would have to increase image
        size by the same factor. For this reason scaling is often modeled not as
        a group, but rather as a semi-group or monoid. This however is not really any
        obstacle -- what matters is that locally it still has structure of a Lie
        group.


        \newcommand{\scalepic}[2]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{scalings/#1}.jpg}
            \end{subfigure}}
        \newcommand{\scalepiccaption}[3]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{scalings/#1}.jpg}
                \caption*{$k=#3$}
            \end{subfigure}}

        \begin{figure}[h]
            \centering
            \foreach \n in {0.5,0.75,1,1.5,2}
                {\scalepic{0temp\n}{0.18}} \\
            \foreach \n in {0.5,0.75,1,1.5,2}
                {\scalepic{121temp\n}{0.18}} \\
            \foreach \n in {0.5,0.75,1,1.5,2}
                {\scalepiccaption{105temp\n}{0.18}{\n}}

            \caption{Images from STL10 dataset with scaled by a factor of $k$.}
            \label{fig:scalings}
        \end{figure}

    \subsubsection{Shear}
        Shear operator `tilts' the coordinates. If we use the standard coordinate
        system centered  in the middle of the image,
        the first vector $e_1$ remains unchanged while $e_2$ is mapped
        to $\lambda e_1 + e_2$:
        \begin{equation}
        \mathcal{SH}_\lambda = \begin{bmatrix}
                1 & \lambda \\
                0 & 1
            \end{bmatrix}
        \end{equation}
        where $\lambda \in \mathbb{R}$. Parameter $\lambda$ is theoretically
        unbounded so the remarks from scaling section apply here as well. In
        practice however $\lambda = \pm -1$ causes tilt of $45 \degree$ which is
        already very significant so we only consider $\lambda \in [-1;1]$.
        Exemplary effect of transformation is shown in figure \ref{fig:shears}.

        \newcommand{\shearpic}[2]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{shears/#1}.jpg}
            \end{subfigure}}
        \newcommand{\shearpiccaption}[3]{
            \begin{subfigure}{#2\textwidth}
                \includegraphics[width=\linewidth]{{shears/#1}.jpg}
                \caption*{$\lambda=#3$}
            \end{subfigure}}

        \begin{figure}[h]
            \centering
            \foreach \n in {-1,-0.5,0,0.5,1}
                {\shearpic{0temp\n}{0.18}} \\
            \foreach \n in {-1,-0.5,0,0.5,1}
                {\shearpic{121temp\n}{0.18}} \\
            \foreach \n in {-1,-0.5,0,0.5,1}
                {\shearpiccaption{105temp\n}{0.18}{\n}}

                \caption{Images from STL10 dataset transformed by
                $\mathcal{SH}_\lambda$ for varying $\lambda$.}
            \label{fig:shears}
        \end{figure}

