\section{Image symmetry implementation}
\newcommand\cbw{\mathit{CBW}}
\newcommand\gbw{\mathit{GBW}}

\subsection{ResNet}
TODO (?)
\subsection{Invariant Layers}
\subsubsection{CBW Layer}
As described in \ref{sec:theoretical_equiinv}, in order to construct a network
invariant to some group of transformations, either the first layer has to be
invariant or first layers have to be equivariant with an invariant layer
following them. In this section we construct networks of both of these types.
First we need to construct layers invariant to changes in
contrast, brightness, color balance and gamma correction as defined in
\ref{sec:transformations}. If possible it would be best to use a single function
invariant to all of these transformations at once. Fortunately such layer
exists. We show that simple instance normalization has this property.
Instance normalization layer $\mathit{CBW}$ transforms each channel
$X_c$ of image $X$ by:
$$ \mathit{CBW}(X_c) = \frac{X_c-E[X_c]}{\sigma(X_c)} $$
where $E[X_c]$ is mean value of $X_c$.
Each instance in batch is normalized separately.
\textit{CBW} is invariant to changes in contrast $\mcc$:
\begin{align*}
    \mathit{CBW}(\mcc_a(X)_c) &=
    \frac{aX_c+(1-a)E[X]_c - E\left[aX_c+(1-a)E[X]_c\right]}{\sigma(aX_c+(1-a)E[X]_c)} \\
    &= \frac{aX_c+(1-a)E[X]_c - aE[X_c]-(1-a)E[X]_c}{a\sigma(X_c)} \\
    &= \frac{aX_c-aE[X_c]}{a\sigma(X_c)} \\
    &= \frac{X_c-E[X_c]}{\sigma(X_c)} \\
    &= \mathit{CBW}(X_c)
\end{align*}
Similarly for brightness changes $\mathcal{B}$:
\begin{align*}
    \mathit{CBW}(\mathcal{B}_a(X)_c) &=
    \frac{aX_c - E\left[aX\right]_c}{\sigma(aX_c)} \\
    &= \frac{aX-aE[X_c]}{a\sigma(X_c)} \\
    &= \frac{X_c-E[X_c]}{\sigma(X_c)} \\
    &= \mathit{CBW}(X_c)
\end{align*}
Whereas color balance changes $\mathcal{W}$ act on individual channels by simple
scaling so:
\begin{align*}
    \mathit{CBW}(\mathcal{W}_T(X)_c) &=
    \frac{T_cX_c - E\left[T_cX_c\right]}{\sigma(T_cX_c)} = \\
    &= \frac{T_cX-T_cE[X_c]}{T_c\sigma(X_c)} = \\
    &= \frac{X_c-E[X_c]}{\sigma(X_c)} = \\
    &= \mathit{CBW}(X_c)
\end{align*}
Therefore any neural network with $\mathit{CBW}$ as first layer
makes the network invariant to considered image transformations.
We denote this type of networks as \{model-name\}+InCBW0, e.g. Plain+InCBW0 or
RotEq+InCBW0. Where 0 refers to placement of the invariant layer at the
beginning of the network. We also consider invariant networks with $\mathit{CBW}$
layer placed further down the processing stream. As mentioned earlier,
constructing such network requires all layers before $\mathit{CBW}$ to be
equivariant. Since the equivariance to changes in color balance is not defined,
we focus on contrast and brightness. In the next section \ref{sec:equ_models} we
present variants of common layers like convolution or pooling equivariant to
these transformations. We use these components to construct invariant networks
denoted InB1, InB2, InB3. Numbers again refer to
placement of the $\mathit{CBW}$ layer -- in InBn the invariant layer is placed
after Resnet's nth BasicBlock.

\subsubsection{GBW Layer}
Just like $\cbw$ layer is invariant to changes in contrast,
we can construct similar normalization layer $\gbw$ invariant to changes in
gamma:
\begin{equation}
    \gbw(X_c) = \frac{\log{X_c}-E[\log{X_c}]}{\sigma(\log{X_c})}
    \label{eq:GBW}
\end{equation}
where $\log(X)$ is entrywise logarithm of $X$. The base of the
logarithm is not very important -- invariant behaviour doesn't depend on it's value.
Now let us assume that we're given an image -- $3\times H \times W$ tensor --
with strictly positive values..
Operators $\mathcal{G}$, $\mathcal{B}$ and $\mathcal{W}$ act on it entrywise, so
we can as well analyze transformation of individual channels of the image.
Let $X$ be a single channel.
We prove \ref{eq:GBW} is invariant to gamma, brightness and color balance
changes. $\mathcal{B}$ and $\mathcal{W}$ transform single channel in the same
way, by multiplication,
so we only need proofs for $\mathcal{G}$ and $\mathcal{B}$:

\begin{align*}
    \gbw(\mathcal{G}_a(X)) &=
    \gbw(X^a) \\
    &= \frac{\log(X^a)-E[\log(X^a)]}{\sigma(\log(X^a))} \\
    &= \frac{a\log(X)-E[a\log(X)]}{\sigma(a\log(X))} \\
    &= \frac{a\log(X)-aE[\log(X)]}{a\sigma(\log(X))} \\
    &= \frac{\log(X)-E[\log(X)]}{\sigma(\log(X))} \\
    &= \gbw(X)
\end{align*}

\begin{align*}
    \gbw(\mathcal{B}_a(X)) &=
    \gbw(aX) \\
    &= \frac{\log(aX)-E[\log(aX)]}{\sigma(\log(aX))} \\
    &= \frac{\log(X)+\log(a)-E[\log(X)+\log(a)]}{\sigma(\log(X)+\log(a))} \\
    &= \frac{\log(X)+\log(a)-E[\log(X)]-\log(a)}{\sigma(\log(X))} \\
    &= \frac{\log(X)-E[\log(X)]}{\sigma(\log(X))} \\
    &= \gbw(X)
\end{align*}

There is however slight technical issue with definition \ref{eq:GBW}.
Since images contain values from interval $[0;1]$,
taking their logarithm directly is impossible because of
cells containing $0$. This problem is solved easily by adding some small constant value
$\epsilon$ to every cell before computing logarithm. The exact definition
is then in fact the following:
\begin{equation}
\gbw_{\epsilon}(X_c) =
    \frac{\log(X_c+\epsilon)-E[\log(X_c+\epsilon)]}{\sigma(\log(X_c+\epsilon))}
\end{equation}
To be precise, this small correction breaks the invariance properties,
%TODO: exact value of error
but fortunately the difference between the two layers is no
greater than $0.5\%$ (see section TODO), so we
can treat $\gbw_{\epsilon}$ as if it was also truly invariant.



\subsection{Equivariant models}
\label{sec:equ_models}

\subsubsection{Brightness equivariance}
We begin with showing that common building blocks of CNNs except for
normalization are equivariant to change of brightness:
\begin{itemize}
    \item \textbf{Convolution} is linear operation so $\mathit{Conv}(aX) =
        a\mathit{Conv}(X)$
    \item \textbf{Pooling} - all commonly used pooling layers are equivariant:
        \begin{itemize}
            \item MaxPooling amounts to taking maximal value of some set and\\
                $\max\{ax_1,ax_2,\cdots,ax_n\}=a\max\{x_1,x_2,\cdots,x_n\}$
            \item AveragePooling is precisely expectation operation $E[X]$ and
                    $E[aX] = aE[X]$
            \item EuclideanNormPooling -
                $\sqrt{(ax_1)^2+\cdots+(ax_n)^2} = a\sqrt{x_1^2+\cdots+x_n^2}$
        \end{itemize}
    \item \textbf{Activation functions} -- equivariant pointwise activation
        function is characterised by equation $f(ax) = af(x)$ for all $a>0$.
        Assuming $f$ is differentiable, $af'(ax) = af'(x)$, so $f'(ax)=f'(x)$ which
        means that $f'(a)=f'(1)$ for $a>0$ and $f'(b)=f'(-1)$ for $b<0$.
        This implies $f$ has form of generalized ReLU function
        $$f(x)=\left\{
            \begin{array}{lll}
                ax & \mbox{for some } a \in \mathbb{R} & \mbox{if } x<0 \\
                bx & \mbox{for some } b \in \mathbb{R} & \mbox{if } x \geq 0
            \end{array}\right.$$
        In particular we can use usual ReLU function.
    \item \textbf{Normalization} As shown above, instance normalization is
        invariant to brightness changes. It's easy to see that it also holds
        for Batch Normalization. This stems mainly from division by
        standard deviation which is equivariant to $\mathcal{B}$. In fact
        the numerator of $\mathit{CBW}$ is also equivariant to $\mathcal{B}_a$:
        $aX_c-E[aX_c]=a(X_c-E[X_c])$, so division by any factor involving $a$
        will break equivariance. For the same reason, we can't use pass the
        numerator through any nonlinear function, e.g. square root.
        Therefore if we want the normalization to zero
        out mean value of input, we must divide by a constant. This might have
        adverse effect on network's training since there is no mechanism to keep
        values flowing through the network bounded, which might easily result in
        gradient explosion. We compare norms of gradients in {\color{red}TODO}
\end{itemize}
\subsubsection{Gamma and contrast equivariance}
\subsubsection{Geometric transformations equivariance}


