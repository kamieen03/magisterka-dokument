\section{Image symmetry implementation}
\newcommand\cbw{\mathit{CBW}}
\newcommand\gbw{\mathit{GBW}}

\subsection{Invariant Layers}
\subsubsection{CBW Layer}
As described in \ref{sec:theoretical_equiinv}, in order to construct a network
invariant to some group of transformations, either the first layer has to be
invariant or first layers have to be equivariant with an invariant layer
following them. In this section we construct networks of both of these types.
First we need to construct layers invariant to changes in
contrast, brightness, color balance and gamma correction as defined in
\ref{sec:transformations}. If possible it would be best to use a single function
invariant to all of these transformations at once. Fortunately such layer
exists. We show that simple instance normalization has this property.
Instance normalization layer $\mathit{CBW}$ transforms each channel
$X_c$ of image $X$ by:
$$ \mathit{CBW}(X_c) = \frac{X_c-E[X_c]}{\sigma(X_c)} $$
where $E[X_c]$ is mean value of $X_c$.
Each instance in batch is normalized separately.
\textit{CBW} is invariant to changes in contrast $\mcc$:
\begin{align*}
    \mathit{CBW}(\mcc_a(X)_c) &=
    \frac{aX_c+(1-a)E[X]_c - E\left[aX_c+(1-a)E[X]_c\right]}{\sigma(aX_c+(1-a)E[X]_c)} \\
    &= \frac{aX_c+(1-a)E[X]_c - aE[X_c]-(1-a)E[X]_c}{a\sigma(X_c)} \\
    &= \frac{aX_c-aE[X_c]}{a\sigma(X_c)} \\
    &= \frac{X_c-E[X_c]}{\sigma(X_c)} \\
    &= \mathit{CBW}(X_c)
\end{align*}
Similarly for brightness changes $\mathcal{B}$:
\begin{align*}
    \mathit{CBW}(\mathcal{B}_a(X)_c) &=
    \frac{aX_c - E\left[aX\right]_c}{\sigma(aX_c)} \\
    &= \frac{aX-aE[X_c]}{a\sigma(X_c)} \\
    &= \frac{X_c-E[X_c]}{\sigma(X_c)} \\
    &= \mathit{CBW}(X_c)
\end{align*}
Whereas color balance changes $\mathcal{W}$ act on individual channels by simple
scaling so:
\begin{align*}
    \mathit{CBW}(\mathcal{W}_T(X)_c) &=
    \frac{T_cX_c - E\left[T_cX_c\right]}{\sigma(T_cX_c)} = \\
    &= \frac{T_cX-T_cE[X_c]}{T_c\sigma(X_c)} = \\
    &= \frac{X_c-E[X_c]}{\sigma(X_c)} = \\
    &= \mathit{CBW}(X_c)
\end{align*}
Therefore any neural network with $\mathit{CBW}$ as first layer
makes the network invariant to considered image transformations.
We denote this type of networks as \{model-name\}+InCBW0, e.g. Plain+InCBW0 or
RotEq+InCBW0. Where 0 refers to placement of the invariant layer at the
beginning of the network. We also consider invariant networks with $\mathit{CBW}$
layer placed further down the processing stream. As mentioned earlier,
constructing such network requires all layers before $\mathit{CBW}$ to be
equivariant. Since the equivariance to changes in color balance is not defined,
we focus on contrast and brightness. In the next section \ref{sec:equ_models} we
present variants of common layers like convolution or pooling equivariant to
these transformations. We use these components to construct invariant networks
denoted InB1, InB2, InB3. Numbers again refer to
placement of the $\mathit{CBW}$ layer -- in InBn the invariant layer is placed
after Resnet's nth BasicBlock.

\subsubsection{GBW Layer}
Just like $\cbw$ layer is invariant to changes in contrast,
we can construct similar normalization layer $\gbw$ invariant to changes in
gamma:
\begin{equation}
    \gbw(X_c) = \frac{\log{X_c}-E[\log{X_c}]}{\sigma(\log{X_c})}
    \label{eq:GBW}
\end{equation}
where $\log(X)$ is entrywise logarithm of $X$. The base of the
logarithm is not very important -- invariant behaviour doesn't depend on it's value.
Now let us assume that we're given an image -- $3\times H \times W$ tensor --
with strictly positive values..
Operators $\mathcal{G}$, $\mathcal{B}$ and $\mathcal{W}$ act on it entrywise, so
we can as well analyze transformation of individual channels of the image.
Let $X$ be a single channel.
We prove \ref{eq:GBW} is invariant to gamma, brightness and color balance
changes. $\mathcal{B}$ and $\mathcal{W}$ transform single channel in the same
way, by multiplication,
so we only need proofs for $\mathcal{G}$ and $\mathcal{B}$:

\begin{align*}
    \gbw(\mathcal{G}_a(X)) &=
    \gbw(X^a) \\
    &= \frac{\log(X^a)-E[\log(X^a)]}{\sigma(\log(X^a))} \\
    &= \frac{a\log(X)-E[a\log(X)]}{\sigma(a\log(X))} \\
    &= \frac{a\log(X)-aE[\log(X)]}{a\sigma(\log(X))} \\
    &= \frac{\log(X)-E[\log(X)]}{\sigma(\log(X))} \\
    &= \gbw(X)
\end{align*}

\begin{align*}
    \gbw(\mathcal{B}_a(X)) &=
    \gbw(aX) \\
    &= \frac{\log(aX)-E[\log(aX)]}{\sigma(\log(aX))} \\
    &= \frac{\log(X)+\log(a)-E[\log(X)+\log(a)]}{\sigma(\log(X)+\log(a))} \\
    &= \frac{\log(X)+\log(a)-E[\log(X)]-\log(a)}{\sigma(\log(X))} \\
    &= \frac{\log(X)-E[\log(X)]}{\sigma(\log(X))} \\
    &= \gbw(X)
\end{align*}

There is however slight technical issue with definition \ref{eq:GBW}.
Since images contain values from interval $[0;1]$,
taking their logarithm directly is impossible because of
cells containing $0$. This problem is solved easily by adding some small constant value
$\epsilon$ to every cell before computing logarithm. The exact definition
is then in fact the following:
\begin{equation}
\gbw_{\epsilon}(X_c) =
    \frac{\log(X_c+\epsilon)-E[\log(X_c+\epsilon)]}{\sigma(\log(X_c+\epsilon))}
\end{equation}
To be precise, this small correction breaks the invariance properties,
%TODO: exact value of error
but fortunately the difference between the two layers is no
greater than $0.5\%$ (see section TODO), so we
can treat $\gbw_{\epsilon}$ as if it was also truly invariant.



\subsection{Equivariant models}
\label{sec:equ_models}

\subsubsection{Brightness equivariance}
Recall that layer $f$ is equivaraint to
change of brightness if $ f(aX) = af(X)$ for ant $a>0$.
We begin with showing that common building blocks of CNNs except for
normalization fulfill this condition:
\begin{itemize}
    \item \textbf{Convolution} is linear operation so $\mathit{Conv}(aX) =
        a\mathit{Conv}(X)$
    \item \textbf{Pooling} - all commonly used pooling layers are equivariant:
        \begin{itemize}
            \item MaxPooling amounts to taking maximal value of some set and\\
                $\max\{ax_1,ax_2,\cdots,ax_n\}=a\max\{x_1,x_2,\cdots,x_n\}$
            \item AveragePooling is precisely expectation operation $E[X]$ and
                    $E[aX] = aE[X]$
            \item EuclideanNormPooling -
                $\sqrt{(ax_1)^2+\cdots+(ax_n)^2} = a\sqrt{x_1^2+\cdots+x_n^2}$
        \end{itemize}
    \item \textbf{Activation functions} -- equivariant pointwise activation
        function is characterised by equation $f(ax) = af(x)$ for all $a>0$.
        Assuming $f$ is differentiable, $af'(ax) = af'(x)$, so $f'(ax)=f'(x)$ which
        means that $f'(a)=f'(1)$ for $a>0$ and $f'(b)=f'(-1)$ for $b<0$.
        This implies $f$ has form of generalized ReLU function
        $$f(x)=\left\{
            \begin{array}{lll}
                ax & \mbox{for some } a \in \mathbb{R} & \mbox{if } x<0 \\
                bx & \mbox{for some } b \in \mathbb{R} & \mbox{if } x \geq 0
            \end{array}\right.$$
        In particular we can use usual ReLU function.
    \item \textbf{Normalization} As shown above, instance normalization is
        invariant to brightness changes. It's easy to see that it also holds
        for Batch Normalization. This stems mainly from division by
        standard deviation which is equivariant to $\mathcal{B}$. In fact
        the numerator of $\mathit{CBW}$ is also equivariant to $\mathcal{B}_a$:
        $aX_c-E[aX_c]=a(X_c-E[X_c])$, so division by any factor involving $a$
        will break equivariance. For the same reason, we can't use pass the
        numerator through any nonlinear function, e.g. square root.
        Therefore if we want the normalization to zero
        out mean value of input, we must divide by a constant. This might have
        adverse effect on network's training since there is no mechanism to keep
        values flowing through the network bounded, which might easily result in
        gradient explosion. We compare norms of gradients in {\color{red}TODO}.
\end{itemize}
And so using common components with standard normalization scheme replaced by
mean normalization we can build model equivariant to brightness changes. In
experiments refer to this type of architecture as Brightness-Eq.

\subsubsection{Geometric transformations equivariance}
\label{sec:geom_eq}

\subsubsection{Gamma and contrast equivariance}
As expected from definitions, operators $\mathcal{C}$ and $\mathcal{G}$ are not
as well-behaved as $\mathcal{B}$. While MaxPooling remains equivariant, the rest
of described above layers doesn't (see experiments in section TODO).
For the rest of this section we construct suitable
convolutional layers similar to those of Lie
GCNNs (section \ref{sec:lie_gcnn}),
activation fucntions and
normalization layers
equivaraint to $\mathcal{C}$ and $\mathcal{G}$.

\begin{itemize}
\item
The crucial part of pipeline is convolution layer. Note that nature of geometric
and lighnig symmetries is fundamentally mathematically different. While the
transforms coordinates or grid of image, the latter group acts posintwise on
values present in the tensor without changing coordinates in any way.
This property renders the implementation used in \ref{sec:geom_eq} unfit.
Where in case of geometric symmetries different transformed locations
on original grid are sampled, in case of lightning symmetries these woulb be the
exact same locations. This in turn would cause creation of identical kernels and
unncesery repetition of computation. Therefore it's obligatory to transoform the
kernels in some other way consistent with abstract equations of lifting and convolution.
We implement it using the transformations from transformation group.
That is if we want the layer to be equivariant with respect to
$\mathcal{G}_c$, then it's kernel $K$ assumes values at gamma level $c$ equal to
base kernel gamma-corrected by factor of $c$:
\begin{equation}
    K(g) = K(x,c) = \mathcal{G}_c(K(x,0))
\end{equation}

\item Because of aggregation of gloabl mean of tensor in \ref{eq:contrast},
    designing a nontrivial pointwise activation function equivaraint to \ref{eq:contrast}
    might be impossible -- value in any given cell would automatically depend on
    values of all other cells. Instead we test numerically equivariance of broad
    range of popular activation functions and find that function Softsign
    defined as $\mathit{Softsign}(x) = \frac{x}{1+|x|}$ performs fairly well.
    Exact measurements can be found in section TODO.

    Operator $\mathcal{G}$ proves to be more managable and exactly equivaraint
    activation function can be found, though in the end it turns out to be
    fairly obvious. If we want $f$ to be equivaraint to \ref{eq:gamma}, then in
    particular it needs to fulfill condition $$f\left(x^c\right) =
    f\left(x\right)^c$$
    for any $x>0$ and
    $c > 0$. To obtain analytic form of $f$
    let $$f(x_0)=y_0$$ for some constants $x_0 \neq 1$ and $y_0$.
    Then $$f\left(x_0^c\right) = y_0^c$$ Now we can treat $x_0^c$ as the argument and set
    $x_0^c = z$, so $$f(z) = y_0^{\log_{x_0}z}$$.
\end{itemize}






